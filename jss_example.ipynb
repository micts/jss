{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovieLens Dataset\n",
    "\n",
    "In this notebook, a simple example is illustrated on how to use MinHashing and LSH to find similar users based on movies they have rated. I will use one of the several datasets provided by [GroupLens](https://grouplens.org/), specifically the [MovieLens 20M Dataset](https://grouplens.org/datasets/movielens/). The dataset contains 20 million ratings applied to 27.000 movies by 138.000 users. It is 190 MB zipped, and 876 MB uncompressed. The zip folder contains several files, but we are only interested on the `ratings.csv`, which is around 533 MB.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `pandas` to read the csv file into a dataframe, and display the first 5 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112486027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112484676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112484819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112484727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112484580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0       1        2     3.5  1112486027\n",
       "1       1       29     3.5  1112484676\n",
       "2       1       32     3.5  1112484819\n",
       "3       1       47     3.5  1112484727\n",
       "4       1       50     3.5  1112484580"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ratings = pd.read_csv('ratings.csv')\n",
    "\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only intersted in two columns, namely, `userId` and `movieId`, which represent the users and movies, respectively. An important thing to note here is that these columns are encoded as integers. For example, the first user corresponds to integer 1, the second user to integer 2, etc. The same holds for `movieId`. Another way to think about it is that the values in `userId` and `movieId` are the row and column indices of a sparse term-incidence matrix, represented for example as a `scipy.sparse` matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert `userId` and `movieId` columns into a numpy array, and store it into a new variable called `data`. The new variable contains the actual data that we will use to find similar users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     1,      2],\n",
       "       [     1,     29],\n",
       "       [     1,     32],\n",
       "       ..., \n",
       "       [138493,  69644],\n",
       "       [138493,  70286],\n",
       "       [138493,  71619]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = ratings.iloc[:, 0:2].values\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to make sure that the values of users and movies start from 0 (the first user/movie correspond to integer 0). This can be achieved easily by subtracting 1 from each row in both columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     0,      1],\n",
       "       [     0,     28],\n",
       "       [     0,     31],\n",
       "       ..., \n",
       "       [138492,  69643],\n",
       "       [138492,  70285],\n",
       "       [138492,  71618]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:, 0] = data[:, 0] - 1\n",
    "data[:, 1] = data[:, 1] - 1\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the class using `jss.MinHashingLSH`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters\n",
      "----------\n",
      "Nr. of Hash Functions: 132\n",
      "Nr. of Bands: 22\n",
      "Nr. of Rows per Band: 6\n",
      "Threshold: 0.5\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "import JaccardSimilaritySearch as jss\n",
    "\n",
    "mhlsh = jss.MinHashingLSH(data, \n",
    "                          threshold=0.5, \n",
    "                          n_bands=22, \n",
    "                          n_rows=6, \n",
    "                          min_set_size=10, \n",
    "                          random_state=10, \n",
    "                          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`threshold` represents the jaccard similarity score for which, all users with similarity greater than `threshold` are considered to be similar. \n",
    "\n",
    "The main parameters of MinHashingLSH algorithm are the number of bands (`n_bands`) and the number of rows per band (`n_rows`). These two define the number of hash functions to use in MinHashing, where `n_hash_functions = n_bands * n_rows`. I will not get into details on what these parameters mean or how to tune them. For details you can refer to the book of [Mining of Massive Datasets](http://www.mmds.org/). Intuitively, for a fixed number of bands, increasing the number of rows per band tends to make the MinHashingLSH algorithm to finish faster, with the expense of finding less similar items. Decreasing the number of rows per band or increasing the number of bands has the opposite effect. For a fixed number of rows per band, increasing the number of bands will make the algorithm slower, but is possible to find more similar pairs of objects. Decreasing the number of bands has the opposite effect. \n",
    "\n",
    "`min_set_size` is a filter parameter that can be used to remove users that have rated a number of movies that is less than `min_set_size`. In other words, remove sets that contain less than `min_set_size` elements. This can be extremely useful in cases where there are many objects/sets that contain only one (or generally few) identical elements. For these sets, the corresponding columns of the signature matrix are identical, and subsequently those objects are always considered as \"candidates\". This means that the Jaccard similarity needs to be computed for those sets, which can be time consuming if there are many sets with one (or few) identical elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MinHashing...\n",
      "Signature Matrix Created. MinHashing Completed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mhlsh.MinHashing(low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Locality Sensitive Hashing...\n",
      "Processing Band: 22/22\n",
      "Locality Sensitive Hashing Completed.\n",
      "\n",
      "15170 Similar Pairs of Items Found.\n"
     ]
    }
   ],
   "source": [
    "mhlsh.LSH()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
